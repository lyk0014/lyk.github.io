---
title: LLaVA o1 思维链推理在视觉模型的尝试
author: LYK
date: 2024-11-18 19:53:00 +0800
categories: [Reading, Paper]
tags: [LLaMA, LLaVA, VLM, o1, CoT, ToT]
pin: true
description: LLaVA-o1 思维链推理在视觉模型的尝试
media_subpath: '/posts/20241118'
---

## Intro  
> 论文：[LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)  

LLaVA-o1，是一个新颖的视觉语言模型, 能够进行多阶段的结构化和自主推理。  
推理过程：总结、标题、推理、结论。  
主要贡献：
1. 创建了带有详细推理注释的LLaVA\-o1\-100k数据集，支持<b>系统化</b>、<b>结构化</b>响应的训练。
2. 提出了一种阶段性beam搜索方法（推理时）  

结论：（11B）超越了更大甚至闭源模型的性能（Gemini-1.5、GPT-4o-mini、LLaMA-3.2-90B-Vision-instruct）


## Question List
- [x] Q1: 什么是视觉语言模型（Vision-Language Models, VLMs）?
- [x] Q2：常见的VLMs有哪些？
- [x] Q3：针对VLMs，常见的评测方法有哪些？
- [x] Q4：LLaVA-o1有哪些亮点？解决了什么问题？
- [x] Q5：LLaVA-o1的主要方法是什么？
- [x] Q6：LLaVA-o1-100k数据集的构建目标、标准、方法是什么？怎么评价数据量是否足够？
- [x] Q7：下一步需要解决的问题？或努力的方向？

## Answer
### Q1：什么是视觉语言模型（Vision-Language Models, VLMs）?
VLM (Vision-Language Model) 指讲视觉和语言信息融合，通过模型学习实现跨模态交互和推理的技术。
主要用于视觉推理或视觉表征。
区别于传统的计算机视觉模型，VLM不受固定类别集或特定任务（如分类或检测）约束，
它们在大量文本和图像标题对的语料上进行预训练，使其能够以自然语言为指示，并泛化至几乎任何类型的视觉任务。
后续延伸到MLLM (Multimodal-Large-Language-Model)，可以支持语音、图像、文本、视频等多模态输入；输出文本、图像、视频等多模态内容。

### Q2：常见的VLMs有哪些？
这里介绍几个主流的：CLIP, BLIP, MiniGPT, LLaVA, Qwen-VL

#### 基于对比学习的方法
- CLIP：采用双编码器结构，基于对比学习来生成图像和文本表征
- BLIP2：采用编码器-解码器结构 (Frozen ImageEncoder, FineTuned Q-Former, Frozen LLM )，基于对比学习训练

#### 基于mask的VLMs  

#### 基于生成的VLM  

#### 基于预训练主干网络的VLM
- MiniGPT： 仅训练一个线性层将ImageEncoder与LLM对齐，无需微调LLM
- LLaVA：Large Language and Vision Assistant，CLIP+LLM训练
  两阶段训练
  1. 跨模态对齐预训练，冻结ViT和LLM，使用图文对训练Adapter
  2. 指令微调，Adapter和LLM都会训练
- Qwen-VL：ViT + Adapter + LLM, 类似LLaVA
  三阶段训练
  1. 图文对预训练，Adapter + ViT
  2. 多任务预训练，Adapter + ViT + LLM
  3. SFT，Adapter + LLM

### Q3：针对VLMs，常见的评测方法有哪些？
1. Mathvista：  
一个用于数学问题解答的数据集，评估模型在数学推理方面的能力。
2. MMMU (MultiModal Machine Understanding)：  
一个多模态机器理解数据集，可能包含多种类型的任务，如视觉问答、图像描述等。
3. ChartQA：  
专注于图表理解的问答数据集，评估模型对图表数据的理解和推理能力。
4. DocVQA (Document Visual Question Answering)：  
文档视觉问答数据集，评估模型在处理文档图像和相关问题时的性能。
5. VQAv2 (Visual Question Answering v2)：  
视觉问答数据集的第二版，包含更复杂的问题和更丰富的图像类型。
6. MM-MT-Bench (MultiModal Machine Translation Benchmark)：
多模态机器翻译基准测试，评估模型在跨语言视觉信息处理方面的能力。
7. LMSys-Vision (Language Model Systems for Vision)：
一个用于评估语言模型在视觉任务上的性能的数据集，可能包括图像描述、视觉问答等任务。
8. GPT-4o Judge：  
使用GPT-4o模型作为评判标准，对模型生成的文本进行评分。

### Q4：LLaVA-o1有哪些亮点？解决了什么问题？
基于LLaMA-3.2-Vision  
**亮点：**
1. 提出了多阶段推理方式，提升推理的结构化和可控性；分为总结、视觉解释、逻辑推理和结论生成四步。  
   类似LLM中的CoT的Thinking，但是是分阶段采样，然后再往下一步走。   
   类似LLM中的ToT。
2. 构建了LLaVA-o1-100k数据集，对后续提升MLLM的推理能力有帮助。

**解决了什么问题？**
1. 解决了VQA领域缺乏具有详细推理过程数据集的问题
2. 多阶段推理是一个创新性的应用，能够对逻辑推理的效果

### Q5：LLaVA-o1的主要方法是什么？
1. 数据集构建：LLaVA-o1-100k数据集
2. 监督式微调（Supervised Fine-Tuning, SFT）：
- 基础模型：Llama-3.2-11B-Vision-Instruct
- 全参数微调：在单个节点上，使用8个H100 GPU
- 结构化推理训练：  
  - 特殊标签：为了支持结构化推理，LLaVA-o1在训练过程中使用了特殊的标签对每个阶段进行标记，如<SUMMARY>...</SUMMARY>、<CAPTION>...</CAPTION>、<REASONING>...</REASONING>和<CONCLUSION>...</CONCLUSION>。
- 阶段级输出设计：模型被训练以适应用户的需求，对于需要简短回答的情况，结论阶段会提供简洁的答案；如果需要详细解释，则提供全面的结论。
- 阶段级束搜索：采用了一种新颖的推理时阶段级束搜索方法，该方法在每个推理阶段生成多个候选结果，并选择最佳结果以继续生成过程。
- 性能优化：实验和迭代：通过在多个基准测试上评估模型性能，并根据结果进行迭代改进，优化模型在各个阶段的表现。

### Q6：LLaVA-o1-100k数据集的构建目标、标准、方法是什么？怎么评价数据量是否足够？

**构建目标：** 实现具有详细推理过程的VQA数据集
**标准：** 准确、可靠
**方法：** 从多个VQA数据集中筛选，然后使用GPT-4o生成指定格式的数据集
**怎么评价数据量是否足够：** 没有明确说明如何确定100k数据量是否足够，但可以推断，研究者可能基于模型的容量和复杂性来估计所需的数据量。对于一个11B参数的模型，100k样本可能已经足够展示模型的推理能力。


### Q7：下一步需要解决的问题？或努力的方向？
- 扩大规模，验证Scaling Law的效果
- 提升推理的深度和复杂度，例如：多步逻辑推理、因果推理等
- 优化推理时的Beam Search方法




## References
1. [66個視覺語言模型VLM經典論文](https://tomohiroliu22.medium.com/66%E5%80%8B%E8%A6%96%E8%A6%BA%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8Bvlm%E7%B6%93%E5%85%B8%E8%AB%96%E6%96%87-f44f280a7f62)
